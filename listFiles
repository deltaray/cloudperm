#!/usr/bin/env python3

from __future__ import print_function
import re

from cloudperm import *

#export processes
import csv
import json

from googleapiclient import discovery
from httplib2 import *

import sqlite3
from sqlite3 import Error

# From https://wiki.python.org/moin/PrintFails
# Python tries to be too pedantic and ends up failing royally.
# So we have to do this so that we can do things like pipe output to other commands.
import sys, codecs, locale;
#sys.stdout = codecs.getwriter(locale.getpreferredencoding())(sys.stdout);


import pprint

#command line stuff 
try:
    import argparse
    parser = argparse.ArgumentParser(parents=[cloudperm_argparser])
    parser.add_argument('--recursive', '-r', action='store_true', help='Recursively list files inside folders')
    parser.add_argument('--max-depth', '-d', type=int, default=1000, help='The maximum depth that recursion can go.')
    parser.add_argument('--exclude-folder', '-E', type=str, default=[], action='append', help='Do not decend into the folder specified by the DocID')
    parser.add_argument('--show-mime-type', '-M', action='store_true', help='Show column for document MIME type')
    parser.add_argument('--long-list', '-l', action='store_true', help='Show owner and date modified columns.')
    parser.add_argument('documents', metavar='DocID', type=str, nargs='*', help='Google Document IDs')
    args = parser.parse_args()
except ImportError:
    args = None

def listFile():
    """List all files and folders recursively under the specified folder id.
    """
    #make database 
    makeDB()

    fieldsep = u' \u2588 ' # Unicode Full block

    folderids = [];

    # Get the arguments for the folders to check or from the config file.
    if args.documents:
        for folderid in args.documents:
            folderids.append(folderid)
    else:
        folderids.append('root')

    pp = pprint.PrettyPrinter(indent=4)

    credentials = get_credentials(args)
    http = credentials.authorize(httplib2.Http())
    service = discovery.build('drive', 'v2', http=http)
    tot_files = 0

    depth = 0;
    if args.recursive:
        depth = args.max_depth # defaults to 1000, which probably means a loop.
        
###########################################################################
    #create a dictionary 
    listFileDict = {}    
    
    # # Get our list of file IDs.
    key = 0

    ##open database connection 
    conn = sqlite3.connect('listFiles.db')
    c = conn.cursor() 

    for start_folder_id in folderids:
        #calling walk_folders from cloudperm.py-> executes query through API 
        returnedfiles = walk_folders(service, start_folder_id, depth, args.exclude_folder)
        if type(returnedfiles) == list:
            for fileitem in returnedfiles:
                #pp.pprint(fileitem)
                path = build_first_path(service,fileitem['id'])
                #need this for writing to files, otherwise weird chatacters are created and will make it more work to use data in .csv/.tsv/.json
                path = path.replace("â–¶", ">")
                #not sure what this is doing exactly (think managing/formatting data)
                if args.show_mime_type:
                    outputline = "{1:<45} {0} {2:<45} {0} {3}\n".format(fieldsep, fileitem['id'], fileitem['mimeType'], path)
                    listFileDict[tot_files]= (fileitem['id'], fileitem['mimeType'], path)
                if args.long_list:
                    ownerlist = ",".join(fileitem['ownerNames'])
                    lastmodified = fileitem['modifiedDate']
                    parent_list = fileitem['parents']
                    parent = parent_list[0]['id']
                    outputline = "{1:<45} {0} {2:<25} {0} {3} {0} {4}\n".format(fieldsep, fileitem['id'], ownerlist,  lastmodified, path)
                    listFileDict[tot_files]= (fileitem['id'], ownerlist,  lastmodified, path)
                    c.execute("INSERT INTO file_update (fileID, fname, modified_time, parent) VALUES (?,?,?,?)", (fileitem['id'], fileitem['title'], lastmodified, parent))
                    conn.commit() 
                else: 
                    outputline = "{1:<45} {0} {2}\n".format(fieldsep, fileitem['id'], path)
                    listFileDict[tot_files]=(fileitem['id'], path)
                
                sys.stdout.write(outputline)
                
                tot_files+=1
                
    print("Total files: " + str(tot_files))

    update_file = '''SELECT * FROM files'''
    updating_file= c.execute(update_file)
    updating_file= c.fetchall()

    nfile = '''SELECT * FROM file_update'''

    new_file= c.execute(nfile)
    new_file= c.fetchall()

    delete = '''SELECT * FROM delete_files'''

    deleted = c.execute(delete)
    deleted = c.fetchall()

    count = 0
    for item in updating_file:
        for file in new_file:
            #holds file from last iteration in count, if it is not "file" then the item has been deleted from the drive
            if item[0] == file[0]:
                count += 1
            if count == 1:
                if item[2] != file[2]:
                    #update time here
                    c.execute("UPDATE files SET modified_time = ? WHERE fileID = ?", (file[2], item[0])) 
        if count == 0:
            ##insert fileID into deleted files table
            if item[0] not in deleted:
                c.execute("INSERT INTO delete_files (fileID) VALUES (?)", (item[0]))
                conn.commit()
        count = 0

    for i in new_file:
        if i in updating_file:
            count +=1 
        if count == 0:
            ##insert file info int files table 
            c.execute("INSERT INTO files (fileID, fname, modified_time, parent) VALUES (?,?,?,?)", (i[0], i[1], i[2], i[3]))
            conn.commit() 

###delete the file_update table 
    c.execute("DROP TABLE file_update")
    conn.commit()

    #write .json file 
    with open('listFiles.json', 'w') as json_file:
        json.dump(listFileDict, json_file)

#csv file create and write
    with open('listFiles.csv', 'w') as csv_file:
        csvwriter = csv.writer(csv_file)
        #create header row???
        #write dictionary to .csv
        for i in range(len(listFileDict)):
            csvwriter.writerow(listFileDict[i])
            i+=1
    
    #create and write TSV files   
    with open('listFiles.tsv', 'w') as tsv_file:
        tsvWriter = csv.writer(tsv_file, delimiter='\t')
        #create header row
        
        #write dictionary to .tsv
        for j in range(len(listFileDict)):
            tsvWriter.writerow(listFileDict[j])
            j+=1
            
    #things to think about adding:    
    #create headers 
    #write out listFileDict data to the csv
    #write out total files 

if __name__ == '__main__':
    listFile()

